{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data_Analytics_Assignment1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yIcsBxtB2QyO",
        "outputId": "91b3d3de-081f-4c99-a92c-8e19f13f6816"
      },
      "source": [
        "!pip install nltk"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86yIQIuS3ZAd"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kQR_aQg5vSk",
        "outputId": "aca28764-6400-4f79-a44d-766ea477d3fd"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "NamesOfFiles = os.listdir('/content/drive/MyDrive/Data Analytics/Assignment 1')\n",
        "content = []\n",
        "print(NamesOfFiles)\n",
        "for name in NamesOfFiles:\n",
        "  f=open('/content/drive/MyDrive/Data Analytics/Assignment 1/' + name, 'r' )\n",
        "  text_of_file = f.read()\n",
        "  content.append(text_of_file)\n",
        "  f.close\n",
        "print(content)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['f1.txt', 'f2.txt', 'f3.txt', 'f4.txt', 'f5.txt', 'f6.txt', 'f7.txt', 'f8.txt', 'f9.txt', 'f10.txt']\n",
            "[\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\", 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\n\\n', 'ontrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old.', 'Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source.', 'Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC.', 'This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.', 'The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested.', 'Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\\n\\n', 'It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout.', \"The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English.\"]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZOWnhKdClXo"
      },
      "source": [
        "from nltk.corpus import PlaintextCorpusReader"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YkAmVwgCnG7",
        "outputId": "cfcf44fe-33ed-4f71-e399-167020bf6855"
      },
      "source": [
        "x=PlaintextCorpusReader('/content/drive/MyDrive/Data Analytics/Assignment 1','.*')\n",
        "x.fileids()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['f1.txt',\n",
              " 'f10.txt',\n",
              " 'f2.txt',\n",
              " 'f3.txt',\n",
              " 'f4.txt',\n",
              " 'f5.txt',\n",
              " 'f6.txt',\n",
              " 'f7.txt',\n",
              " 'f8.txt',\n",
              " 'f9.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QG3SBDtODMI0",
        "outputId": "a69a7fe8-dc3c-484e-f3ec-67015a21d299"
      },
      "source": [
        "x.words()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Lorem', 'Ipsum', 'is', 'simply', 'dummy', 'text', ...]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlH9xUQuEAr7",
        "outputId": "dec509c1-1fa8-488e-d0fc-add09e1bec82"
      },
      "source": [
        "content1=[]\n",
        "for name in x.fileids():\n",
        "  content1.append(x.raw(fileids=name))\n",
        "print(content1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\", \"The point of using Lorem Ipsum is that it has a more-or-less normal distribution of letters, as opposed to using 'Content here, content here', making it look like readable English.\", 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\r\\n\\r\\n', 'ontrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old.', 'Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source.', 'Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of \"de Finibus Bonorum et Malorum\" (The Extremes of Good and Evil) by Cicero, written in 45 BC.', 'This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, \"Lorem ipsum dolor sit amet..\", comes from a line in section 1.10.32.', 'The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested.', 'Sections 1.10.32 and 1.10.33 from \"de Finibus Bonorum et Malorum\" by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\\r\\n\\r\\n', 'It is a long established fact that a reader will be distracted by the readable content of a page when looking at its layout.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFY2GkRXJBt5",
        "outputId": "8a1f3250-eaed-40db-bb92-239cdd12e89a"
      },
      "source": [
        "normalize=[]\n",
        "for i in range(len(content)):\n",
        "  normalize.append(' '.join([word.lower() for word in content[i].split() if word.isalpha()]))\n",
        "print(normalize)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lorem ipsum is simply dummy text of the printing and typesetting lorem ipsum has been the standard dummy text ever since the when an unknown printer took a galley of type and scrambled it to make a type specimen', 'it has survived not only five but also the leap into electronic remaining essentially it was popularised in the with the release of letraset sheets containing lorem ipsum and more recently with desktop publishing software like aldus pagemaker including versions of lorem', 'ontrary to popular lorem ipsum is not simply random it has roots in a piece of classical latin literature from making it over years', 'richard a latin professor at college in looked up one of the more obscure latin from a lorem ipsum and going through the cites of the word in classical discovered the undoubtable', 'lorem ipsum comes from sections and of finibus bonorum et extremes of good and by written in', 'this book is a treatise on the theory of very popular during the the first line of lorem ipsum dolor sit comes from a line in section', 'the standard chunk of lorem ipsum used since the is reproduced below for those', 'sections and from finibus bonorum et by cicero are also reproduced in their exact original accompanied by english versions from the translation by', 'it is a long established fact that a reader will be distracted by the readable content of a page when looking at its', 'the point of using lorem ipsum is that it has a normal distribution of as opposed to using content making it look like readable']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JscS8v0K6OP"
      },
      "source": [
        "from nltk import word_tokenize"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENfA9IqoLkUt",
        "outputId": "521e683d-db75-426d-d29e-c1f661cf801a"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywbPd5f9Lc8l",
        "outputId": "3c7c6921-205c-4eb6-d703-9f57888cfe30"
      },
      "source": [
        "tokenize=[]\n",
        "for word in normalize:\n",
        "  tokenize.append(nltk.word_tokenize(word))\n",
        "print(tokenize)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['lorem', 'ipsum', 'is', 'simply', 'dummy', 'text', 'of', 'the', 'printing', 'and', 'typesetting', 'lorem', 'ipsum', 'has', 'been', 'the', 'standard', 'dummy', 'text', 'ever', 'since', 'the', 'when', 'an', 'unknown', 'printer', 'took', 'a', 'galley', 'of', 'type', 'and', 'scrambled', 'it', 'to', 'make', 'a', 'type', 'specimen'], ['it', 'has', 'survived', 'not', 'only', 'five', 'but', 'also', 'the', 'leap', 'into', 'electronic', 'remaining', 'essentially', 'it', 'was', 'popularised', 'in', 'the', 'with', 'the', 'release', 'of', 'letraset', 'sheets', 'containing', 'lorem', 'ipsum', 'and', 'more', 'recently', 'with', 'desktop', 'publishing', 'software', 'like', 'aldus', 'pagemaker', 'including', 'versions', 'of', 'lorem'], ['ontrary', 'to', 'popular', 'lorem', 'ipsum', 'is', 'not', 'simply', 'random', 'it', 'has', 'roots', 'in', 'a', 'piece', 'of', 'classical', 'latin', 'literature', 'from', 'making', 'it', 'over', 'years'], ['richard', 'a', 'latin', 'professor', 'at', 'college', 'in', 'looked', 'up', 'one', 'of', 'the', 'more', 'obscure', 'latin', 'from', 'a', 'lorem', 'ipsum', 'and', 'going', 'through', 'the', 'cites', 'of', 'the', 'word', 'in', 'classical', 'discovered', 'the', 'undoubtable'], ['lorem', 'ipsum', 'comes', 'from', 'sections', 'and', 'of', 'finibus', 'bonorum', 'et', 'extremes', 'of', 'good', 'and', 'by', 'written', 'in'], ['this', 'book', 'is', 'a', 'treatise', 'on', 'the', 'theory', 'of', 'very', 'popular', 'during', 'the', 'the', 'first', 'line', 'of', 'lorem', 'ipsum', 'dolor', 'sit', 'comes', 'from', 'a', 'line', 'in', 'section'], ['the', 'standard', 'chunk', 'of', 'lorem', 'ipsum', 'used', 'since', 'the', 'is', 'reproduced', 'below', 'for', 'those'], ['sections', 'and', 'from', 'finibus', 'bonorum', 'et', 'by', 'cicero', 'are', 'also', 'reproduced', 'in', 'their', 'exact', 'original', 'accompanied', 'by', 'english', 'versions', 'from', 'the', 'translation', 'by'], ['it', 'is', 'a', 'long', 'established', 'fact', 'that', 'a', 'reader', 'will', 'be', 'distracted', 'by', 'the', 'readable', 'content', 'of', 'a', 'page', 'when', 'looking', 'at', 'its'], ['the', 'point', 'of', 'using', 'lorem', 'ipsum', 'is', 'that', 'it', 'has', 'a', 'normal', 'distribution', 'of', 'as', 'opposed', 'to', 'using', 'content', 'making', 'it', 'look', 'like', 'readable']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZn1pAGVLyrN"
      },
      "source": [
        "from nltk.corpus import *"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXgbZcbrMWen",
        "outputId": "33875638-6bdc-4c8a-bc5f-452ba7b4c7a6"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words=nltk.corpus.stopwords.words(fileids='english')\n",
        "no_stop_words=[]\n",
        "for i in tokenize:\n",
        "    no_stop_words.append([word for word in i if word not in stop_words])\n",
        "print(no_stop_words)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[['lorem', 'ipsum', 'simply', 'dummy', 'text', 'printing', 'typesetting', 'lorem', 'ipsum', 'standard', 'dummy', 'text', 'ever', 'since', 'unknown', 'printer', 'took', 'galley', 'type', 'scrambled', 'make', 'type', 'specimen'], ['survived', 'five', 'also', 'leap', 'electronic', 'remaining', 'essentially', 'popularised', 'release', 'letraset', 'sheets', 'containing', 'lorem', 'ipsum', 'recently', 'desktop', 'publishing', 'software', 'like', 'aldus', 'pagemaker', 'including', 'versions', 'lorem'], ['ontrary', 'popular', 'lorem', 'ipsum', 'simply', 'random', 'roots', 'piece', 'classical', 'latin', 'literature', 'making', 'years'], ['richard', 'latin', 'professor', 'college', 'looked', 'one', 'obscure', 'latin', 'lorem', 'ipsum', 'going', 'cites', 'word', 'classical', 'discovered', 'undoubtable'], ['lorem', 'ipsum', 'comes', 'sections', 'finibus', 'bonorum', 'et', 'extremes', 'good', 'written'], ['book', 'treatise', 'theory', 'popular', 'first', 'line', 'lorem', 'ipsum', 'dolor', 'sit', 'comes', 'line', 'section'], ['standard', 'chunk', 'lorem', 'ipsum', 'used', 'since', 'reproduced'], ['sections', 'finibus', 'bonorum', 'et', 'cicero', 'also', 'reproduced', 'exact', 'original', 'accompanied', 'english', 'versions', 'translation'], ['long', 'established', 'fact', 'reader', 'distracted', 'readable', 'content', 'page', 'looking'], ['point', 'using', 'lorem', 'ipsum', 'normal', 'distribution', 'opposed', 'using', 'content', 'making', 'look', 'like', 'readable']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7sWUt-u0NkzA",
        "outputId": "7ce296ad-209d-486f-9f0d-6743be21f485"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "final=[]\n",
        "for i in no_stop_words:\n",
        "    final.append(' '.join([ps.stem(word) for word in i]))\n",
        "print(final)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['lorem ipsum simpli dummi text print typeset lorem ipsum standard dummi text ever sinc unknown printer took galley type scrambl make type specimen', 'surviv five also leap electron remain essenti popularis releas letraset sheet contain lorem ipsum recent desktop publish softwar like aldu pagemak includ version lorem', 'ontrari popular lorem ipsum simpli random root piec classic latin literatur make year', 'richard latin professor colleg look one obscur latin lorem ipsum go cite word classic discov undoubt', 'lorem ipsum come section finibu bonorum et extrem good written', 'book treatis theori popular first line lorem ipsum dolor sit come line section', 'standard chunk lorem ipsum use sinc reproduc', 'section finibu bonorum et cicero also reproduc exact origin accompani english version translat', 'long establish fact reader distract readabl content page look', 'point use lorem ipsum normal distribut oppos use content make look like readabl']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl1wrWuvhafK",
        "outputId": "9dd6b082-7285-4da9-91fb-e8c573af4227"
      },
      "source": [
        "#bow using inbuilt functions\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer()\n",
        "X=cv.fit_transform(final)\n",
        "print(X.toarray())\n",
        "print(cv.get_feature_names())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 2\n",
            "  0 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 1 1 0 0 1 1 0 2 0 1 0 0 2 1 0 1 0 0 0 0 0]\n",
            " [0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1\n",
            "  0 1 1 1 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 1 1 0 0 0 0\n",
            "  0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
            "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1\n",
            "  2 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 0 0 1 0 1\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1\n",
            "  0 0 0 0 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
            "  0 0 0 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0]]\n",
            "['accompani', 'aldu', 'also', 'bonorum', 'book', 'chunk', 'cicero', 'cite', 'classic', 'colleg', 'come', 'contain', 'content', 'desktop', 'discov', 'distract', 'distribut', 'dolor', 'dummi', 'electron', 'english', 'essenti', 'establish', 'et', 'ever', 'exact', 'extrem', 'fact', 'finibu', 'first', 'five', 'galley', 'go', 'good', 'includ', 'ipsum', 'latin', 'leap', 'letraset', 'like', 'line', 'literatur', 'long', 'look', 'lorem', 'make', 'normal', 'obscur', 'one', 'ontrari', 'oppos', 'origin', 'page', 'pagemak', 'piec', 'point', 'popular', 'popularis', 'print', 'printer', 'professor', 'publish', 'random', 'readabl', 'reader', 'recent', 'releas', 'remain', 'reproduc', 'richard', 'root', 'scrambl', 'section', 'sheet', 'simpli', 'sinc', 'sit', 'softwar', 'specimen', 'standard', 'surviv', 'text', 'theori', 'took', 'translat', 'treatis', 'type', 'typeset', 'undoubt', 'unknown', 'use', 'version', 'word', 'written', 'year']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eMrdxd0lpmw"
      },
      "source": [
        "import pandas as pd\n",
        "bow=pd.DataFrame(data=X.toarray(),columns=cv.get_feature_names(),index=NamesOfFiles)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlbpuASwmI8X",
        "outputId": "65a3ca6b-849c-4322-cf8b-c24b16eb22e3"
      },
      "source": [
        "print(bow)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         accompani  aldu  also  bonorum  ...  version  word  written  year\n",
            "f1.txt           0     0     0        0  ...        0     0        0     0\n",
            "f2.txt           0     1     1        0  ...        1     0        0     0\n",
            "f3.txt           0     0     0        0  ...        0     0        0     1\n",
            "f4.txt           0     0     0        0  ...        0     1        0     0\n",
            "f5.txt           0     0     0        1  ...        0     0        1     0\n",
            "f6.txt           0     0     0        0  ...        0     0        0     0\n",
            "f7.txt           0     0     0        0  ...        0     0        0     0\n",
            "f8.txt           1     0     1        1  ...        1     0        0     0\n",
            "f9.txt           0     0     0        0  ...        0     0        0     0\n",
            "f10.txt          0     0     0        0  ...        0     0        0     0\n",
            "\n",
            "[10 rows x 95 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5laXrxEmmMr",
        "outputId": "964e715d-18ef-402e-c5d8-fab7ab01ed06"
      },
      "source": [
        "#Implementation of bag of words without using inbuilt functions\n",
        "doc_count={}\n",
        "for i in range(len(final)):\n",
        "  word_count={}\n",
        "  for word in final[i].split():\n",
        "    if word not in word_count.keys():\n",
        "      word_count[word]=0\n",
        "    word_count[word]+=1\n",
        "  doc_count[NamesOfFiles[i]]=word_count\n",
        "print(doc_count)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'f1.txt': {'lorem': 2, 'ipsum': 2, 'simpli': 1, 'dummi': 2, 'text': 2, 'print': 1, 'typeset': 1, 'standard': 1, 'ever': 1, 'sinc': 1, 'unknown': 1, 'printer': 1, 'took': 1, 'galley': 1, 'type': 2, 'scrambl': 1, 'make': 1, 'specimen': 1}, 'f2.txt': {'surviv': 1, 'five': 1, 'also': 1, 'leap': 1, 'electron': 1, 'remain': 1, 'essenti': 1, 'popularis': 1, 'releas': 1, 'letraset': 1, 'sheet': 1, 'contain': 1, 'lorem': 2, 'ipsum': 1, 'recent': 1, 'desktop': 1, 'publish': 1, 'softwar': 1, 'like': 1, 'aldu': 1, 'pagemak': 1, 'includ': 1, 'version': 1}, 'f3.txt': {'ontrari': 1, 'popular': 1, 'lorem': 1, 'ipsum': 1, 'simpli': 1, 'random': 1, 'root': 1, 'piec': 1, 'classic': 1, 'latin': 1, 'literatur': 1, 'make': 1, 'year': 1}, 'f4.txt': {'richard': 1, 'latin': 2, 'professor': 1, 'colleg': 1, 'look': 1, 'one': 1, 'obscur': 1, 'lorem': 1, 'ipsum': 1, 'go': 1, 'cite': 1, 'word': 1, 'classic': 1, 'discov': 1, 'undoubt': 1}, 'f5.txt': {'lorem': 1, 'ipsum': 1, 'come': 1, 'section': 1, 'finibu': 1, 'bonorum': 1, 'et': 1, 'extrem': 1, 'good': 1, 'written': 1}, 'f6.txt': {'book': 1, 'treatis': 1, 'theori': 1, 'popular': 1, 'first': 1, 'line': 2, 'lorem': 1, 'ipsum': 1, 'dolor': 1, 'sit': 1, 'come': 1, 'section': 1}, 'f7.txt': {'standard': 1, 'chunk': 1, 'lorem': 1, 'ipsum': 1, 'use': 1, 'sinc': 1, 'reproduc': 1}, 'f8.txt': {'section': 1, 'finibu': 1, 'bonorum': 1, 'et': 1, 'cicero': 1, 'also': 1, 'reproduc': 1, 'exact': 1, 'origin': 1, 'accompani': 1, 'english': 1, 'version': 1, 'translat': 1}, 'f9.txt': {'long': 1, 'establish': 1, 'fact': 1, 'reader': 1, 'distract': 1, 'readabl': 1, 'content': 1, 'page': 1, 'look': 1}, 'f10.txt': {'point': 1, 'use': 2, 'lorem': 1, 'ipsum': 1, 'normal': 1, 'distribut': 1, 'oppos': 1, 'content': 1, 'make': 1, 'look': 1, 'like': 1, 'readabl': 1}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efQIEtJlpA8l",
        "outputId": "ede4b120-1691-4c19-b3a8-00bbdd117a1e"
      },
      "source": [
        "bow_self=pd.DataFrame(data=doc_count)\n",
        "print(bow_self)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "           f1.txt  f2.txt  f3.txt  f4.txt  ...  f7.txt  f8.txt  f9.txt  f10.txt\n",
            "lorem         2.0     2.0     1.0     1.0  ...     1.0     NaN     NaN      1.0\n",
            "ipsum         2.0     1.0     1.0     1.0  ...     1.0     NaN     NaN      1.0\n",
            "simpli        1.0     NaN     1.0     NaN  ...     NaN     NaN     NaN      NaN\n",
            "dummi         2.0     NaN     NaN     NaN  ...     NaN     NaN     NaN      NaN\n",
            "text          2.0     NaN     NaN     NaN  ...     NaN     NaN     NaN      NaN\n",
            "...           ...     ...     ...     ...  ...     ...     ...     ...      ...\n",
            "page          NaN     NaN     NaN     NaN  ...     NaN     NaN     1.0      NaN\n",
            "point         NaN     NaN     NaN     NaN  ...     NaN     NaN     NaN      1.0\n",
            "normal        NaN     NaN     NaN     NaN  ...     NaN     NaN     NaN      1.0\n",
            "distribut     NaN     NaN     NaN     NaN  ...     NaN     NaN     NaN      1.0\n",
            "oppos         NaN     NaN     NaN     NaN  ...     NaN     NaN     NaN      1.0\n",
            "\n",
            "[95 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "_6Ym7S2EpeZZ",
        "outputId": "ef593f5f-640d-4d1d-ac1e-da584cdcff31"
      },
      "source": [
        "bow_self.fillna(0,inplace=True)\n",
        "bow_self"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1.txt</th>\n",
              "      <th>f2.txt</th>\n",
              "      <th>f3.txt</th>\n",
              "      <th>f4.txt</th>\n",
              "      <th>f5.txt</th>\n",
              "      <th>f6.txt</th>\n",
              "      <th>f7.txt</th>\n",
              "      <th>f8.txt</th>\n",
              "      <th>f9.txt</th>\n",
              "      <th>f10.txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lorem</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ipsum</th>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simpli</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dummi</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>page</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>point</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>normal</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>distribut</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oppos</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>95 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           f1.txt  f2.txt  f3.txt  f4.txt  ...  f7.txt  f8.txt  f9.txt  f10.txt\n",
              "lorem         2.0     2.0     1.0     1.0  ...     1.0     0.0     0.0      1.0\n",
              "ipsum         2.0     1.0     1.0     1.0  ...     1.0     0.0     0.0      1.0\n",
              "simpli        1.0     0.0     1.0     0.0  ...     0.0     0.0     0.0      0.0\n",
              "dummi         2.0     0.0     0.0     0.0  ...     0.0     0.0     0.0      0.0\n",
              "text          2.0     0.0     0.0     0.0  ...     0.0     0.0     0.0      0.0\n",
              "...           ...     ...     ...     ...  ...     ...     ...     ...      ...\n",
              "page          0.0     0.0     0.0     0.0  ...     0.0     0.0     1.0      0.0\n",
              "point         0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0      1.0\n",
              "normal        0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0      1.0\n",
              "distribut     0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0      1.0\n",
              "oppos         0.0     0.0     0.0     0.0  ...     0.0     0.0     0.0      1.0\n",
              "\n",
              "[95 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO5NZaBWpxBv",
        "outputId": "4ac5a720-ef6a-42f8-de53-4acba2842f15"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tf_idf=TfidfVectorizer(smooth_idf=False,norm=False)\n",
        "Y=tf_idf.fit_transform(final)\n",
        "print(Y.toarray())\n",
        "print(tf_idf.get_feature_names())"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  6.60517019 0.         0.         0.         0.         0.\n",
            "  3.30258509 0.         0.         0.         0.         0.\n",
            "  0.         3.30258509 0.         0.         0.         2.4462871\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         2.4462871  2.2039728  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         3.30258509 3.30258509\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         3.30258509\n",
            "  0.         0.         2.60943791 2.60943791 0.         0.\n",
            "  3.30258509 2.60943791 0.         6.60517019 0.         3.30258509\n",
            "  0.         0.         6.60517019 3.30258509 0.         3.30258509\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         3.30258509 2.60943791 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         3.30258509\n",
            "  0.         3.30258509 0.         0.         0.         0.\n",
            "  0.         3.30258509 0.         3.30258509 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  3.30258509 0.         0.         0.         3.30258509 1.22314355\n",
            "  0.         3.30258509 3.30258509 2.60943791 0.         0.\n",
            "  0.         0.         2.4462871  0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         3.30258509\n",
            "  0.         0.         0.         3.30258509 0.         0.\n",
            "  0.         3.30258509 0.         0.         0.         3.30258509\n",
            "  3.30258509 3.30258509 0.         0.         0.         0.\n",
            "  0.         3.30258509 0.         0.         0.         3.30258509\n",
            "  0.         0.         3.30258509 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         2.60943791 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         2.60943791 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.22314355\n",
            "  2.60943791 0.         0.         0.         0.         3.30258509\n",
            "  0.         0.         1.22314355 2.2039728  0.         0.\n",
            "  0.         3.30258509 0.         0.         0.         0.\n",
            "  3.30258509 0.         2.60943791 0.         0.         0.\n",
            "  0.         0.         3.30258509 0.         0.         0.\n",
            "  0.         0.         0.         0.         3.30258509 0.\n",
            "  0.         0.         2.60943791 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         3.30258509]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         3.30258509 2.60943791 3.30258509 0.         0.\n",
            "  0.         0.         3.30258509 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         3.30258509 0.         0.         1.22314355\n",
            "  5.21887582 0.         0.         0.         0.         0.\n",
            "  0.         2.2039728  1.22314355 0.         0.         3.30258509\n",
            "  3.30258509 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  3.30258509 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         3.30258509 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         3.30258509 0.\n",
            "  0.         0.         3.30258509 0.         0.        ]\n",
            " [0.         0.         0.         2.60943791 0.         0.\n",
            "  0.         0.         0.         0.         2.60943791 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         2.60943791\n",
            "  0.         0.         3.30258509 0.         2.60943791 0.\n",
            "  0.         0.         0.         3.30258509 0.         1.22314355\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.22314355 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  2.2039728  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         3.30258509 0.        ]\n",
            " [0.         0.         0.         0.         3.30258509 0.\n",
            "  0.         0.         0.         0.         2.60943791 0.\n",
            "  0.         0.         0.         0.         0.         3.30258509\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         3.30258509\n",
            "  0.         0.         0.         0.         0.         1.22314355\n",
            "  0.         0.         0.         0.         6.60517019 0.\n",
            "  0.         0.         1.22314355 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         2.60943791 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  2.2039728  0.         0.         0.         3.30258509 0.\n",
            "  0.         0.         0.         0.         3.30258509 0.\n",
            "  0.         3.30258509 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         3.30258509\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.22314355\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         1.22314355 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         2.60943791 0.         0.         0.\n",
            "  0.         0.         0.         2.60943791 0.         0.\n",
            "  0.         2.60943791 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  2.60943791 0.         0.         0.         0.        ]\n",
            " [3.30258509 0.         2.60943791 2.60943791 0.         0.\n",
            "  3.30258509 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         3.30258509 0.         0.         2.60943791\n",
            "  0.         3.30258509 0.         0.         2.60943791 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         3.30258509 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         2.60943791 0.         0.         0.\n",
            "  2.2039728  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  3.30258509 0.         0.         0.         0.         0.\n",
            "  0.         2.60943791 0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  2.60943791 0.         0.         3.30258509 0.         0.\n",
            "  0.         0.         0.         0.         3.30258509 0.\n",
            "  0.         0.         0.         3.30258509 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  3.30258509 2.2039728  0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         3.30258509 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         2.60943791 3.30258509 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  2.60943791 0.         0.         0.         3.30258509 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         1.22314355\n",
            "  0.         0.         0.         2.60943791 0.         0.\n",
            "  0.         2.2039728  1.22314355 2.2039728  3.30258509 0.\n",
            "  0.         0.         3.30258509 0.         0.         0.\n",
            "  0.         3.30258509 0.         0.         0.         0.\n",
            "  0.         0.         0.         2.60943791 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  5.21887582 0.         0.         0.         0.        ]]\n",
            "['accompani', 'aldu', 'also', 'bonorum', 'book', 'chunk', 'cicero', 'cite', 'classic', 'colleg', 'come', 'contain', 'content', 'desktop', 'discov', 'distract', 'distribut', 'dolor', 'dummi', 'electron', 'english', 'essenti', 'establish', 'et', 'ever', 'exact', 'extrem', 'fact', 'finibu', 'first', 'five', 'galley', 'go', 'good', 'includ', 'ipsum', 'latin', 'leap', 'letraset', 'like', 'line', 'literatur', 'long', 'look', 'lorem', 'make', 'normal', 'obscur', 'one', 'ontrari', 'oppos', 'origin', 'page', 'pagemak', 'piec', 'point', 'popular', 'popularis', 'print', 'printer', 'professor', 'publish', 'random', 'readabl', 'reader', 'recent', 'releas', 'remain', 'reproduc', 'richard', 'root', 'scrambl', 'section', 'sheet', 'simpli', 'sinc', 'sit', 'softwar', 'specimen', 'standard', 'surviv', 'text', 'theori', 'took', 'translat', 'treatis', 'type', 'typeset', 'undoubt', 'unknown', 'use', 'version', 'word', 'written', 'year']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwrb0Iidq0XS"
      },
      "source": [
        "import pandas as pd\n",
        "tf=pd.DataFrame(data=Y.toarray(),columns=tf_idf.get_feature_names(),index=NamesOfFiles)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNedRHT-rXLn",
        "outputId": "1aa76ef9-8d7f-4402-d183-7efbeec24a73"
      },
      "source": [
        "print(tf)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "         accompani      aldu      also  ...      word   written      year\n",
            "f1.txt    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
            "f2.txt    0.000000  3.302585  2.609438  ...  0.000000  0.000000  0.000000\n",
            "f3.txt    0.000000  0.000000  0.000000  ...  0.000000  0.000000  3.302585\n",
            "f4.txt    0.000000  0.000000  0.000000  ...  3.302585  0.000000  0.000000\n",
            "f5.txt    0.000000  0.000000  0.000000  ...  0.000000  3.302585  0.000000\n",
            "f6.txt    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
            "f7.txt    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
            "f8.txt    3.302585  0.000000  2.609438  ...  0.000000  0.000000  0.000000\n",
            "f9.txt    0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
            "f10.txt   0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000\n",
            "\n",
            "[10 rows x 95 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch3iWzN8zEvL",
        "outputId": "6045f450-72b1-41c0-d6b7-91811572e5e9"
      },
      "source": [
        "#tf-idf without using inbuilt functions\n",
        "\n",
        "#calculating tf\n",
        "\n",
        "tf_doc={}\n",
        "for i in doc_count.keys():\n",
        "  total=0\n",
        "  tf_word={}\n",
        "  for j in doc_count[i].values():\n",
        "    total+=j\n",
        "  for k in doc_count[i].keys():\n",
        "    tf_word[k]=doc_count[i][k]/total\n",
        "  tf_doc[i]=tf_word\n",
        "print(tf_doc)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'f1.txt': {'lorem': 0.08695652173913043, 'ipsum': 0.08695652173913043, 'simpli': 0.043478260869565216, 'dummi': 0.08695652173913043, 'text': 0.08695652173913043, 'print': 0.043478260869565216, 'typeset': 0.043478260869565216, 'standard': 0.043478260869565216, 'ever': 0.043478260869565216, 'sinc': 0.043478260869565216, 'unknown': 0.043478260869565216, 'printer': 0.043478260869565216, 'took': 0.043478260869565216, 'galley': 0.043478260869565216, 'type': 0.08695652173913043, 'scrambl': 0.043478260869565216, 'make': 0.043478260869565216, 'specimen': 0.043478260869565216}, 'f2.txt': {'surviv': 0.041666666666666664, 'five': 0.041666666666666664, 'also': 0.041666666666666664, 'leap': 0.041666666666666664, 'electron': 0.041666666666666664, 'remain': 0.041666666666666664, 'essenti': 0.041666666666666664, 'popularis': 0.041666666666666664, 'releas': 0.041666666666666664, 'letraset': 0.041666666666666664, 'sheet': 0.041666666666666664, 'contain': 0.041666666666666664, 'lorem': 0.08333333333333333, 'ipsum': 0.041666666666666664, 'recent': 0.041666666666666664, 'desktop': 0.041666666666666664, 'publish': 0.041666666666666664, 'softwar': 0.041666666666666664, 'like': 0.041666666666666664, 'aldu': 0.041666666666666664, 'pagemak': 0.041666666666666664, 'includ': 0.041666666666666664, 'version': 0.041666666666666664}, 'f3.txt': {'ontrari': 0.07692307692307693, 'popular': 0.07692307692307693, 'lorem': 0.07692307692307693, 'ipsum': 0.07692307692307693, 'simpli': 0.07692307692307693, 'random': 0.07692307692307693, 'root': 0.07692307692307693, 'piec': 0.07692307692307693, 'classic': 0.07692307692307693, 'latin': 0.07692307692307693, 'literatur': 0.07692307692307693, 'make': 0.07692307692307693, 'year': 0.07692307692307693}, 'f4.txt': {'richard': 0.0625, 'latin': 0.125, 'professor': 0.0625, 'colleg': 0.0625, 'look': 0.0625, 'one': 0.0625, 'obscur': 0.0625, 'lorem': 0.0625, 'ipsum': 0.0625, 'go': 0.0625, 'cite': 0.0625, 'word': 0.0625, 'classic': 0.0625, 'discov': 0.0625, 'undoubt': 0.0625}, 'f5.txt': {'lorem': 0.1, 'ipsum': 0.1, 'come': 0.1, 'section': 0.1, 'finibu': 0.1, 'bonorum': 0.1, 'et': 0.1, 'extrem': 0.1, 'good': 0.1, 'written': 0.1}, 'f6.txt': {'book': 0.07692307692307693, 'treatis': 0.07692307692307693, 'theori': 0.07692307692307693, 'popular': 0.07692307692307693, 'first': 0.07692307692307693, 'line': 0.15384615384615385, 'lorem': 0.07692307692307693, 'ipsum': 0.07692307692307693, 'dolor': 0.07692307692307693, 'sit': 0.07692307692307693, 'come': 0.07692307692307693, 'section': 0.07692307692307693}, 'f7.txt': {'standard': 0.14285714285714285, 'chunk': 0.14285714285714285, 'lorem': 0.14285714285714285, 'ipsum': 0.14285714285714285, 'use': 0.14285714285714285, 'sinc': 0.14285714285714285, 'reproduc': 0.14285714285714285}, 'f8.txt': {'section': 0.07692307692307693, 'finibu': 0.07692307692307693, 'bonorum': 0.07692307692307693, 'et': 0.07692307692307693, 'cicero': 0.07692307692307693, 'also': 0.07692307692307693, 'reproduc': 0.07692307692307693, 'exact': 0.07692307692307693, 'origin': 0.07692307692307693, 'accompani': 0.07692307692307693, 'english': 0.07692307692307693, 'version': 0.07692307692307693, 'translat': 0.07692307692307693}, 'f9.txt': {'long': 0.1111111111111111, 'establish': 0.1111111111111111, 'fact': 0.1111111111111111, 'reader': 0.1111111111111111, 'distract': 0.1111111111111111, 'readabl': 0.1111111111111111, 'content': 0.1111111111111111, 'page': 0.1111111111111111, 'look': 0.1111111111111111}, 'f10.txt': {'point': 0.07692307692307693, 'use': 0.15384615384615385, 'lorem': 0.07692307692307693, 'ipsum': 0.07692307692307693, 'normal': 0.07692307692307693, 'distribut': 0.07692307692307693, 'oppos': 0.07692307692307693, 'content': 0.07692307692307693, 'make': 0.07692307692307693, 'look': 0.07692307692307693, 'like': 0.07692307692307693, 'readabl': 0.07692307692307693}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xvxSCc24kUU",
        "outputId": "e01d14d4-c787-404d-9ed9-a91347e254d3"
      },
      "source": [
        "#calculating idf matrix\n",
        "import math\n",
        "idf_doc={}\n",
        "for i in doc_count.keys():\n",
        "  for j in doc_count[i].keys():\n",
        "    if j not in idf_doc.keys():\n",
        "      idf_doc[j]=0\n",
        "    idf_doc[j]+=1\n",
        "for k in idf_doc.keys():\n",
        "  val=idf_doc[k]\n",
        "  idf_doc[k]=math.log10(total_docs/val)\n",
        "print(idf_doc)\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'lorem': 0.09691001300805642, 'ipsum': 0.09691001300805642, 'simpli': 0.6989700043360189, 'dummi': 1.0, 'text': 1.0, 'print': 1.0, 'typeset': 1.0, 'standard': 0.6989700043360189, 'ever': 1.0, 'sinc': 0.6989700043360189, 'unknown': 1.0, 'printer': 1.0, 'took': 1.0, 'galley': 1.0, 'type': 1.0, 'scrambl': 1.0, 'make': 0.5228787452803376, 'specimen': 1.0, 'surviv': 1.0, 'five': 1.0, 'also': 0.6989700043360189, 'leap': 1.0, 'electron': 1.0, 'remain': 1.0, 'essenti': 1.0, 'popularis': 1.0, 'releas': 1.0, 'letraset': 1.0, 'sheet': 1.0, 'contain': 1.0, 'recent': 1.0, 'desktop': 1.0, 'publish': 1.0, 'softwar': 1.0, 'like': 0.6989700043360189, 'aldu': 1.0, 'pagemak': 1.0, 'includ': 1.0, 'version': 0.6989700043360189, 'ontrari': 1.0, 'popular': 0.6989700043360189, 'random': 1.0, 'root': 1.0, 'piec': 1.0, 'classic': 0.6989700043360189, 'latin': 0.6989700043360189, 'literatur': 1.0, 'year': 1.0, 'richard': 1.0, 'professor': 1.0, 'colleg': 1.0, 'look': 0.5228787452803376, 'one': 1.0, 'obscur': 1.0, 'go': 1.0, 'cite': 1.0, 'word': 1.0, 'discov': 1.0, 'undoubt': 1.0, 'come': 0.6989700043360189, 'section': 0.5228787452803376, 'finibu': 0.6989700043360189, 'bonorum': 0.6989700043360189, 'et': 0.6989700043360189, 'extrem': 1.0, 'good': 1.0, 'written': 1.0, 'book': 1.0, 'treatis': 1.0, 'theori': 1.0, 'first': 1.0, 'line': 1.0, 'dolor': 1.0, 'sit': 1.0, 'chunk': 1.0, 'use': 0.6989700043360189, 'reproduc': 0.6989700043360189, 'cicero': 1.0, 'exact': 1.0, 'origin': 1.0, 'accompani': 1.0, 'english': 1.0, 'translat': 1.0, 'long': 1.0, 'establish': 1.0, 'fact': 1.0, 'reader': 1.0, 'distract': 1.0, 'readabl': 0.6989700043360189, 'content': 0.6989700043360189, 'page': 1.0, 'point': 1.0, 'normal': 1.0, 'distribut': 1.0, 'oppos': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3loDVt179YZV",
        "outputId": "23fbe269-a227-4a9a-af78-e97e072335e8"
      },
      "source": [
        "for i in tf_doc.keys():\n",
        "  for j in tf_doc[i].keys():\n",
        "    tf_doc[i][j]*=idf_doc[j]\n",
        "print(tf_doc)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'f1.txt': {'lorem': 0.0059826091185755555, 'ipsum': 0.0059826091185755555, 'simpli': 0.03042016206461139, 'dummi': 0.08695652173913043, 'text': 0.08695652173913043, 'print': 0.043478260869565216, 'typeset': 0.043478260869565216, 'standard': 0.03124581347167429, 'ever': 0.043478260869565216, 'sinc': 0.03124581347167429, 'unknown': 0.043478260869565216, 'printer': 0.043478260869565216, 'took': 0.043478260869565216, 'galley': 0.043478260869565216, 'type': 0.08695652173913043, 'scrambl': 0.043478260869565216, 'make': 0.015901560324987952, 'specimen': 0.043478260869565216}, 'f2.txt': {'surviv': 0.041666666666666664, 'five': 0.041666666666666664, 'also': 0.027295200010659746, 'leap': 0.041666666666666664, 'electron': 0.041666666666666664, 'remain': 0.041666666666666664, 'essenti': 0.041666666666666664, 'popularis': 0.041666666666666664, 'releas': 0.041666666666666664, 'letraset': 0.041666666666666664, 'sheet': 0.041666666666666664, 'contain': 0.041666666666666664, 'lorem': 0.005733333738634908, 'ipsum': 0.002866666869317454, 'recent': 0.041666666666666664, 'desktop': 0.041666666666666664, 'publish': 0.041666666666666664, 'softwar': 0.041666666666666664, 'like': 0.020356627790062257, 'aldu': 0.041666666666666664, 'pagemak': 0.041666666666666664, 'includ': 0.041666666666666664, 'version': 0.027295200010659746}, 'f3.txt': {'ontrari': 0.07692307692307693, 'popular': 0.05311843339343473, 'lorem': 0.005292308066432223, 'ipsum': 0.005292308066432223, 'simpli': 0.053820286729697076, 'random': 0.07692307692307693, 'root': 0.07692307692307693, 'piec': 0.07692307692307693, 'classic': 0.05364419012172787, 'latin': 0.05364419012172787, 'literatur': 0.07692307692307693, 'make': 0.028133529805747917, 'year': 0.07692307692307693}, 'f4.txt': {'richard': 0.0625, 'latin': 0.08717180894780778, 'professor': 0.0625, 'colleg': 0.0625, 'look': 0.025157470583328567, 'one': 0.0625, 'obscur': 0.0625, 'lorem': 0.004300000303976181, 'ipsum': 0.004300000303976181, 'go': 0.0625, 'cite': 0.0625, 'word': 0.0625, 'classic': 0.04358590447390389, 'discov': 0.0625, 'undoubt': 0.0625}, 'f5.txt': {'lorem': 0.006880000486361889, 'ipsum': 0.006880000486361889, 'come': 0.06905396341146514, 'section': 0.04860185735604326, 'finibu': 0.0655084800255834, 'bonorum': 0.0655084800255834, 'et': 0.0655084800255834, 'extrem': 0.1, 'good': 0.1, 'written': 0.1}, 'f6.txt': {'book': 0.07692307692307693, 'treatis': 0.07692307692307693, 'theori': 0.07692307692307693, 'popular': 0.05311843339343473, 'first': 0.07692307692307693, 'line': 0.15384615384615385, 'lorem': 0.005292308066432223, 'ipsum': 0.005292308066432223, 'dolor': 0.07692307692307693, 'sit': 0.07692307692307693, 'come': 0.05311843339343473, 'section': 0.03738604412003328}, 'f7.txt': {'standard': 0.10266481569264409, 'chunk': 0.14285714285714285, 'lorem': 0.009828572123374128, 'ipsum': 0.009828572123374128, 'use': 0.0697941524230706, 'sinc': 0.10266481569264409, 'reproduc': 0.09358354289369056}, 'f8.txt': {'section': 0.03738604412003328, 'finibu': 0.05039113848121799, 'bonorum': 0.05039113848121799, 'et': 0.05039113848121799, 'cicero': 0.07692307692307693, 'also': 0.05039113848121799, 'reproduc': 0.05039113848121799, 'exact': 0.07692307692307693, 'origin': 0.07692307692307693, 'accompani': 0.07692307692307693, 'english': 0.07692307692307693, 'version': 0.05039113848121799, 'translat': 0.07692307692307693}, 'f9.txt': {'long': 0.1111111111111111, 'establish': 0.1111111111111111, 'fact': 0.1111111111111111, 'reader': 0.1111111111111111, 'distract': 0.1111111111111111, 'readabl': 0.054284340773499355, 'content': 0.054284340773499355, 'page': 0.1111111111111111, 'look': 0.04472439214813967}, 'f10.txt': {'point': 0.07692307692307693, 'use': 0.07516293337869143, 'lorem': 0.005292308066432223, 'ipsum': 0.005292308066432223, 'normal': 0.07692307692307693, 'distribut': 0.07692307692307693, 'oppos': 0.07692307692307693, 'content': 0.03758146668934571, 'make': 0.028133529805747917, 'look': 0.03096304071794285, 'like': 0.03758146668934571, 'readabl': 0.03758146668934571}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADkHRp0S-GKr",
        "outputId": "fcac71b4-6fa2-488d-930f-a6321923d797"
      },
      "source": [
        "tf_idf_self=pd.DataFrame(data=tf_doc)\n",
        "print(tf_idf_self)"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             f1.txt    f2.txt    f3.txt  ...  f8.txt    f9.txt   f10.txt\n",
            "lorem      0.005983  0.005733  0.005292  ...     NaN       NaN  0.005292\n",
            "ipsum      0.005983  0.002867  0.005292  ...     NaN       NaN  0.005292\n",
            "simpli     0.030420       NaN  0.053820  ...     NaN       NaN       NaN\n",
            "dummi      0.086957       NaN       NaN  ...     NaN       NaN       NaN\n",
            "text       0.086957       NaN       NaN  ...     NaN       NaN       NaN\n",
            "...             ...       ...       ...  ...     ...       ...       ...\n",
            "page            NaN       NaN       NaN  ...     NaN  0.111111       NaN\n",
            "point           NaN       NaN       NaN  ...     NaN       NaN  0.076923\n",
            "normal          NaN       NaN       NaN  ...     NaN       NaN  0.076923\n",
            "distribut       NaN       NaN       NaN  ...     NaN       NaN  0.076923\n",
            "oppos           NaN       NaN       NaN  ...     NaN       NaN  0.076923\n",
            "\n",
            "[95 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "Y5uF37zg-dpf",
        "outputId": "a1ced537-449f-4c36-84e3-6c3b23196452"
      },
      "source": [
        "tf_idf_self.fillna(0,inplace=True)\n",
        "tf_idf_self"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1.txt</th>\n",
              "      <th>f2.txt</th>\n",
              "      <th>f3.txt</th>\n",
              "      <th>f4.txt</th>\n",
              "      <th>f5.txt</th>\n",
              "      <th>f6.txt</th>\n",
              "      <th>f7.txt</th>\n",
              "      <th>f8.txt</th>\n",
              "      <th>f9.txt</th>\n",
              "      <th>f10.txt</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>lorem</th>\n",
              "      <td>0.005983</td>\n",
              "      <td>0.005733</td>\n",
              "      <td>0.005292</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.00688</td>\n",
              "      <td>0.005292</td>\n",
              "      <td>0.009829</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ipsum</th>\n",
              "      <td>0.005983</td>\n",
              "      <td>0.002867</td>\n",
              "      <td>0.005292</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.00688</td>\n",
              "      <td>0.005292</td>\n",
              "      <td>0.009829</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>simpli</th>\n",
              "      <td>0.030420</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053820</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dummi</th>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>text</th>\n",
              "      <td>0.086957</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>page</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>point</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>normal</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>distribut</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>oppos</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.076923</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>95 rows Ã— 10 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             f1.txt    f2.txt    f3.txt  ...  f8.txt    f9.txt   f10.txt\n",
              "lorem      0.005983  0.005733  0.005292  ...     0.0  0.000000  0.005292\n",
              "ipsum      0.005983  0.002867  0.005292  ...     0.0  0.000000  0.005292\n",
              "simpli     0.030420  0.000000  0.053820  ...     0.0  0.000000  0.000000\n",
              "dummi      0.086957  0.000000  0.000000  ...     0.0  0.000000  0.000000\n",
              "text       0.086957  0.000000  0.000000  ...     0.0  0.000000  0.000000\n",
              "...             ...       ...       ...  ...     ...       ...       ...\n",
              "page       0.000000  0.000000  0.000000  ...     0.0  0.111111  0.000000\n",
              "point      0.000000  0.000000  0.000000  ...     0.0  0.000000  0.076923\n",
              "normal     0.000000  0.000000  0.000000  ...     0.0  0.000000  0.076923\n",
              "distribut  0.000000  0.000000  0.000000  ...     0.0  0.000000  0.076923\n",
              "oppos      0.000000  0.000000  0.000000  ...     0.0  0.000000  0.076923\n",
              "\n",
              "[95 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    }
  ]
}